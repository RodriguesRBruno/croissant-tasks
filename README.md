# Croissant Tasks Examples
This repository contains standarized Croissant Task examples for two different use cases: A Benchmark, loosely based on the [MLCommons AILuminate Benchmark](https://mlcommons.org/ailuminate/), and a data processing task. Both examples are hypothetical and do **NOT** constitute complete use cases. For both use cases, both a Croissant Task Problem and a Croissant Task Solution example are provided. Someone can define a Croissant Task Problem, leaving in specifications for data, assets and/or implementations that must be provided as part of corresponding Task Solutions. This Task Problem may then be shared with a large community, from which other users can generate their own Croissant Task Problem solutions. Data, assets and/or implementations from the Solutions should conform to the specification of the Croissant Task Problem, even though they will differ in each Solution.

## DICOM To NIfTI
The [Croissant Task Problem](/DICOM%20To%20NIfTI/dicom2nifti-taskproblem.md) and [Croissant Task Solution](/DICOM%20To%20NIfTI/dicom2nifti-tasksolution.md) are available in the [DICOM To NIfTI subdirectory.](/DICOM%20To%20NIfTI/). This is a hypothetical data preparation task that requires specific formats for the input and output data, but does not specify an implementation. Therefore, solutions must provide an implementation for the DICOM to NIfTI conversion, as well as input and output data used for this conversion. The form of the implementation itself is not specified. It could be a simple Python function, a Docker container or anything else, as long as the input and output data are coherent with the specification in the Task Problem. Other assumptions are included in the Task Problem/Solution files.

## Benchmark
The Benchmark example is loosely based on the [MLCommons AILuminate Benchmark](https://mlcommons.org/ailuminate/) Benchmark. The [Croissant Task Problem](/Benchmark/benchmark-taskproblem.md) and [Croissant Task Solution](./Benchmark/benchmark-tasksolution.md) are available in the [Benchmark subdirectory](/Benchmark/). This is a Benchmark test for Language models. Therefore, the input is a model, which is an asset, not data. There are no specific outputs in terms of data or assets. However, an evaluation is performed on the model, for which metrics are collected. Sample data from the publicly availble [AILuminate Demo Dataset](https://github.com/mlcommons/ailuminate) is also included, which can be used for developing and debugging potential input models. Other assumptions are included in the Task Problem/Solution files.